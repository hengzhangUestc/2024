# 国际大模型

## 量化后失真怎么办？

## LLM 中的 kv Cache

## Q K V的内容，这些要深入理解，不要一知半解，这样回答不好

## agent 工程， ms架构，能不能搞一下

## 学习 llm 原理等相关内容

## 搜推领域的 bert模型更新

## lookahead 如何并行推理，真的是 预测下一组数据吗

在计算存在大量冗余的情况下，我们可以通过计算换带宽，我们可以类比CPU中的指令预取流程，不需要等到上一条指令已经执行完的时候在取下一条指令，而是根据分支预测猜测可能的指令，
然后提前取指及译码。如果实际执行结果跟预取的不一致，那就重新取正确的指令执行，但是如果成功了，就直接用译码的结果，降低了延迟时间。 同样的，在大模型推理过程中，
为了加速推理，我们通过预判断的方式来计算多个词，举例来说: 假如 4 5两个词经常一起出现，我们在前一次的前向过程中，已经根据1 2 3计算出来了下一个词是4， 
那我们就把1234和12345一起去计算，分别得到1234和12345作为上下文的预测的下一个词，如果1234预测的下一个词刚好是5，那我们就直接用5预测的下一个词作为起始词来计算，
这样一次就计算了两个词，相比只计算一个词效率更高。如果1234预测出来的下一个词不是5，那就把中间状态回滚掉，从1234预测的下一个词开始计算。状态回滚的耗时低于前向的耗时，如果命中概率很高，就会带来加速


## lp是什么？集团和蚂蚁内部


## soft_search 这些要不要挖掘具体的内容。


## cuda 的一些简单的知识点，hitByKey  或者  buildSeq 如何利用  cuda 化，这部分要讲一下。

## 为什么要用 tritonserver 这个推理服务框架，而不用别的推理服务框架，有做相应的对比吗？

## pytorch 是用来干什么的
